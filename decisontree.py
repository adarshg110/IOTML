# -*- coding: utf-8 -*-
"""Untitled5.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SlYo27Lhevr8IR0YKbNy6LXD98901_VQ
"""

import pandas as pd
import tensorflow as tf
from tensorflow import keras
from keras import layers
from sklearn.model_selection import train_test_split

import psutil

initial_memory = psutil.virtual_memory().used / (1024 ** 2)  # in MB

df=pd.read_csv('/content/ashrae_db2.01.csv', engine='python')

df.shape

cond1=df['Building type']=='Office'
#cond2=dff['Country']=='India'
df=df[cond1]

df['Building type'].value_counts()

df['Koppen climate classification'].unique()

df.columns
#climate,season,sex,Theraml sensation,Theramal prefernce,PMV,PPD,SET,CLO,MET,AIR TEMP(C),Relaice hum,Air velcoity(m/s),outdoor monthly air temperature(C))

df=df.loc[:,['Cooling startegy_building level','Climate','Season','Country','Thermal sensation','Clo','Air temperature (C)','Relative humidity (%)','Air velocity (m/s)','Met']]

#df.drop(['PMV','PPD','SET','Sex'],axis=1,inplace=True)
#df.dropna(inplace=True)
null_counts=df.isnull().sum()

print(null_counts)

# Drop rows with null values in the specified column
df.dropna(subset=['Cooling startegy_building level'],inplace=True)
df.dropna(subset=['Season'],inplace=True)
df.dropna(subset=['Thermal sensation'],inplace=True)

df_scaled=df.loc[:,['Cooling startegy_building level','Climate','Season','Country']]

from sklearn.preprocessing import LabelEncoder

label_encoder = LabelEncoder()

for column in df_scaled.columns:
    if df_scaled[column].dtype == 'object':  # Check if the column contains categorical data
        df_scaled[f'{column}_LabelEncoded'] = label_encoder.fit_transform(df_scaled[column])

df_scaled.drop(['Cooling startegy_building level','Climate','Season','Country'],inplace=True,axis=1)

df.drop(['Cooling startegy_building level','Climate','Season','Country'],inplace=True,axis=1)
df=pd.concat([df,df_scaled],axis=1)

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
# Fit and transform the data
standardized_data = scaler.fit_transform(df)

# Create a DataFrame with the standardized data
df= pd.DataFrame(standardized_data, columns=df.columns)

df

#use knn impuuter on feautres which are null to stop loosing data during data cleaning

from sklearn.impute import KNNImputer
impute_knn=KNNImputer()
imputed_df=impute_knn.fit_transform(df[['Relative humidity (%)','Met','Air temperature (C)','Air velocity (m/s)','Clo']])

imputed_df

df[['Relative humidity (%)','Met','Air temperature (C)','Air velocity (m/s)','Clo']]=imputed_df

X=df.loc[:,['Clo', 'Air temperature (C)','Relative humidity (%)', 'Air velocity (m/s)', 'Met','Cooling startegy_building level_LabelEncoded', 'Climate_LabelEncoded','Season_LabelEncoded', 'Country_LabelEncoded']]
y=df['Thermal sensation']

'''
import numpy as np
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor

X=df.loc[:,['Clo', 'Air temperature (C)','Relative humidity (%)', 'Air velocity (m/s)', 'Met','Cooling startegy_building level_LabelEncoded', 'Climate_LabelEncoded','Season_LabelEncoded', 'Country_LabelEncoded']]
y=df['Thermal sensation']


model = RandomForestRegressor()  # Adjust alpha as needed for regularization strength
model.fit(X, y)

# Get coefficients (weights) and corresponding feature
'''

#df.drop('Thermal sensation',axis=1,inplace=True)

df.columns

feature_importances = model.feature_importances_
feature_names = df.columns

sorted_indices = np.argsort(feature_importances)[::-1]
sorted_importances = feature_importances[sorted_indices]
sorted_names = feature_names[sorted_indices]

threshold = 0.15 * sorted_importances[0]

# Filter features based on the threshold
selected_indices = np.where(sorted_importances > threshold)[0]
selected_importances = sorted_importances[selected_indices]
selected_names = sorted_names[selected_indices]

# Plot selected features
plt.figure(figsize=(30, 6))
plt.bar(range(len(selected_importances)), selected_importances, tick_label=selected_names)
plt.title('Selected Features with Importance > 5% of the Highest Weighted Feature')
plt.xlabel('Feature')
plt.ylabel('Importance')
plt.show()

df.columns

X=df.loc[:,['Clo', 'Air temperature (C)','Relative humidity (%)', 'Air velocity (m/s)', 'Met','Cooling startegy_building level_LabelEncoded', 'Climate_LabelEncoded','Season_LabelEncoded', 'Country_LabelEncoded']]
y=df['Thermal sensation']

from sklearn.model_selection import train_test_split

x_train,x_test,y_train,y_test=train_test_split(X,y,test_size=0.1,random_state=46)

from sklearn.ensemble import RandomForestRegressor
model1=RandomForestRegressor(max_depth=5,max_features=1.0,min_samples_leaf=1,min_samples_split=2,n_estimators=185)

model1.fit(x_train,y_train)

from sklearn.metrics import mean_squared_error
#Best Hyperparameters: OrderedDict([('max_depth', 5), ('max_features', 1.0), ('min_samples_leaf', 1), ('min_samples_split', 2), ('n_estimators', 185)])
#Best Mean Squared Error: 0.0976514595592057

x_pred=model1.predict(x_train)
tloss=mean_squared_error(x_pred,y_train)

print('training loss:',tloss)

y_pred=model1.predict(x_test)
loss=mean_squared_error(y_pred,y_test)
print('test loss:',loss)

from sklearn.model_selection import KFold
import numpy as np
from sklearn.metrics import mean_absolute_error

kf = KFold(n_splits=5, shuffle=True, random_state=42)
mae_scores = []

for train_index, test_index in kf.split(X):
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]

    # Fit the model
    model1.fit(X_train, y_train)

    # Make predictions on the test set
    y_pred = model1.predict(X_test)

    # Compute Mean Squared Error (MSE)
    mae = mean_absolute_error(y_test, y_pred)
    mae_scores.append(mae)

# Display MSE scores for each fold
for i, mse in enumerate(mae_scores):
    print(f'Fold {i+1} MSE: {mae}')

# Average MSE across all folds
average_mae = np.mean(mae_scores)
print(f'Average MAE: {average_mae}')

#!pip install scikit-optimize

'''
from sklearn.model_selection import cross_val_score
from skopt import BayesSearchCV


# Define the objective function to be minimized (negative mean cross-validated score)
def objective_function(params):
    # Convert hyperparameters to int where necessary
    params['n_estimators'] = int(params['n_estimators'])
    params['max_depth'] = int(params['max_depth'])

    # Create a Random Forest Regressor with the given hyperparameters
    rf_regressor = RandomForestRegressor(**params, random_state=42)

    # Calculate the negative mean cross-validated score (minimize)
    score = -np.mean(cross_val_score(rf_regressor, X, y, cv=5, scoring='mean_squared_error'))

    return score

# Define the search space for hyperparameters
param_space = {
    'n_estimators': (10, 200),
    'max_depth': (1, 20),
    'min_samples_split': (2, 20),
    'min_samples_leaf': (1, 20),
    'max_features': (0.1, 1.0)
}

# Perform Bayesian optimization
bayes_search = BayesSearchCV(
    RandomForestRegressor(),
    param_space,
    n_iter=50,  # Number of optimization steps
    cv=5,       # Number of cross-validation folds
    n_jobs=-1,  # Use all available CPU cores
    random_state=42
)

# Run the optimization
bayes_search.fit(X, y)

# Print the best hyperparameters and the corresponding score
print("Best Hyperparameters:", bayes_search.best_params_)
print("Best Mean Squared Error:", bayes_search.best_score_)
'''

from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
import numpy as np

# Load a sample regression dataset


# Access individual trees in the Random Forest
individual_trees = model1.estimators_

# Choose one tree (for example, the first tree in the forest)
tree_to_prune = individual_trees[0]

# Get the cost-complexity pruning path
path = tree_to_prune.cost_complexity_pruning_path(X, y)

# Extract the effective alphas and impurities
alphas = path['ccp_alphas']
impurities = path['impurities']

# Plot the impurities as a function of alpha
import matplotlib.pyplot as plt
plt.plot(alphas[:-1], impurities[:-1], marker='o', drawstyle="steps-post")
plt.xlabel("Effective alpha")
plt.ylabel("Total impurity of leaves")
plt.title("Total Impurity vs. Effective Alpha for pruning")
plt.show()

# Find the best alpha that minimizes impurity
best_alpha = alphas[np.argmax(-impurities)]

# Prune the tree using the best alpha
pruned_tree = DecisionTreeRegressor(ccp_alpha=best_alpha, random_state=42)
pruned_tree.fit(X, y)

x_train,x_test,y_train,y_test=train_test_split(X,y,test_size=0.1,random_state=46)

x_pred=pruned_tree.predict(x_train)
tloss=mean_absolute_error(x_pred,y_train)
print('training loss',tloss)

y_pred=pruned_tree.predict(x_test)
loss=mean_absolute_error(y_pred,y_test)
print('test loss:',loss)

import pickle

# Save the model
with open('pruned.pkl', 'wb') as model_file:
    pickle.dump(model1, model_file)

with open('pruned.pkl', 'rb') as model_file:
    loaded_model = pickle.load(model_file)

df.drop('Thermal sensation',inplace=True,axis=1)

input_data=df.sample(100)

input_data

# Warm-up runs
for _ in range(5):
    pruned_tree.predict(input_data) #warm up the pretrained model

import time

# Measure inference time
num_runs = 200# Number of inference runs to measure
inference_times = []

for _ in range(num_runs):
    start_time = time.time()
    pruned_tree.predict(input_data)
    end_time = time.time()
    inference_time = end_time - start_time
    inference_times.append(inference_time)

# Calculate statistics
average_inference_time = sum(inference_times) / num_runs
min_inference_time = min(inference_times)
max_inference_time = max(inference_times)

print(f"Average Inference Time: {average_inference_time} seconds")
print(f"Min Inference Time: {min_inference_time} seconds")
print(f"Max Inference Time: {max_inference_time} seconds")

import psutil

# CPU Usage
cpu_percent = psutil.cpu_percent(interval=1)

# Memory Usage
memory_info = psutil.virtual_memory()
total_memory_mb = memory_info.total / (1024 ** 2)
available_memory_mb = memory_info.available / (1024 ** 2)
used_memory_mb = memory_info.used / (1024 ** 2)

# Disk Usage (for the root directory '/')
disk_info = psutil.disk_usage('/')
total_disk_space_mb = disk_info.total / (1024 ** 2)
used_disk_space_mb = disk_info.used / (1024 ** 2)
free_disk_space_mb = disk_info.free / (1024 ** 2)

# Network Usage (bytes sent and received)
network_info = psutil.net_io_counters()
bytes_sent_mb = network_info.bytes_sent / (1024 ** 2)
bytes_received_mb = network_info.bytes_recv / (1024 ** 2)

# Print the results
print(f"CPU Usage: {cpu_percent}%")
print(f"Total Memory: {total_memory_mb:.2f} MB")
print(f"Available Memory: {available_memory_mb:.2f} MB")
print(f"Used Memory: {used_memory_mb:.2f} MB")
print(f"Total Disk Space: {total_disk_space_mb:.2f} MB")
print(f"Used Disk Space: {used_disk_space_mb:.2f} MB")
print(f"Free Disk Space: {free_disk_space_mb:.2f} MB")
print(f"Bytes Sent: {bytes_sent_mb:.2f} MB")
print(f"Bytes Received: {bytes_received_mb:.2f} MB")

import psutil

# Get the initial memory usage
#initial_memory = psutil.virtual_memory().used / (1024 ** 2)  # in MB

# Code for running your model
# ...

# Get the memory usage after running your model
final_memory = psutil.virtual_memory().used / (1024 ** 2)  # in MB

# Calculate the memory usage increase during model execution
memory_usage_increase = final_memory - initial_memory

print(f"Initial Memory Usage: {initial_memory:.2f} MB")
print(f"Final Memory Usage: {final_memory:.2f} MB")
print(f"Memory Usage Increase: {memory_usage_increase:.2f} MB")

'''
CPU Usage: 18.1%
Total Memory: 12982.62 MB
Available Memory: 11190.64 MB
Used Memory: 1502.75 MB
Total Disk Space: 110300.25 MB
Used Disk Space: 26914.14 MB
Free Disk Space: 83370.11 MB
Bytes Sent: 33.91 MB
Bytes Received: 65.58 MB
'''