# -*- coding: utf-8 -*-
"""ANN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16sOpsPILNs8r-uPppXkZQT9HEAnAkDOD
"""

import pandas as pd
import tensorflow as tf
from tensorflow import keras
from keras import layers
from keras.models import Sequential
from keras.layers import Dense,Flatten
from keras.models import load_model
from keras.layers import Dropout
from sklearn.model_selection import train_test_split

import psutil

initial_memory = psutil.virtual_memory().used / (1024 ** 2)  # in MB

df=pd.read_csv('/content/ashrae_db2.01.csv', engine='python')

df.shape

cond1=df['Building type']=='Office'
#cond2=dff['Country']=='India'
df=df[cond1]

df['Building type'].value_counts()

df['Koppen climate classification'].unique()

df.columns
#climate,season,sex,Theraml sensation,Theramal prefernce,PMV,PPD,SET,CLO,MET,AIR TEMP(C),Relaice hum,Air velcoity(m/s),outdoor monthly air temperature(C))

df=df.loc[:,['Cooling startegy_building level','Climate','Season','Country','Thermal sensation','Clo','Air temperature (C)','Relative humidity (%)','Air velocity (m/s)','Met']]

#df.drop(['PMV','PPD','SET','Sex'],axis=1,inplace=True)
#df.dropna(inplace=True)
null_counts=df.isnull().sum()

print(null_counts)

# Drop rows with null values in the specified column
df.dropna(subset=['Cooling startegy_building level'],inplace=True)
df.dropna(subset=['Season'],inplace=True)
df.dropna(subset=['Thermal sensation'],inplace=True)

null_counts=df.isnull().sum()

print(null_counts)

df_scaled=df.loc[:,['Cooling startegy_building level','Climate','Season','Country']]

from sklearn.preprocessing import LabelEncoder

label_encoder = LabelEncoder()

for column in df_scaled.columns:
    if df_scaled[column].dtype == 'object':  # Check if the column contains categorical data
        df_scaled[f'{column}_LabelEncoded'] = label_encoder.fit_transform(df_scaled[column])

df_scaled.drop(['Cooling startegy_building level','Climate','Season','Country'],inplace=True,axis=1)

df.drop(['Cooling startegy_building level','Climate','Season','Country'],inplace=True,axis=1)
df=pd.concat([df,df_scaled],axis=1)

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
# Fit and transform the data
standardized_data = scaler.fit_transform(df)

# Create a DataFrame with the standardized data
df= pd.DataFrame(standardized_data, columns=df.columns)

df

#use knn impuuter on feautres which are null to stop loosing data during data cleaning

from sklearn.impute import KNNImputer
impute_knn=KNNImputer()
imputed_df=impute_knn.fit_transform(df[['Relative humidity (%)','Met','Air temperature (C)','Air velocity (m/s)','Clo']])

imputed_df

df[['Relative humidity (%)','Met','Air temperature (C)','Air velocity (m/s)','Clo']]=imputed_df

X=df.loc[:,['Clo', 'Air temperature (C)','Relative humidity (%)', 'Air velocity (m/s)', 'Met','Cooling startegy_building level_LabelEncoded', 'Climate_LabelEncoded','Season_LabelEncoded', 'Country_LabelEncoded']]
y=df['Thermal sensation']

x_train,x_test,y_train,y_test=train_test_split(X,y,test_size=0.1,random_state=46)

x_train.shape

x_train.shape

input_shape=x_train.shape

import numpy as np
input_shape=(58266,9)

model =  keras.Sequential(
    [
    layers.Dense(units=128,input_dim=9,activation="tanh"),
    layers.Dropout(0.2),
    layers.Dense(units=64,activation="tanh"),
    layers.Dropout(0.2),
    layers.Dense(units=32,activation="tanh"),
    layers.Dropout(0.2),
    layers.Dense(units=16,activation="tanh"),
    layers.Dropout(0.2),
    layers.Dense(units=8,activation="tanh"),
    layers.Dense(units=1,activation="linear")
    ]
)
model.summary()

# Adjust alpha as needed for regularization strength\
model.compile(optimizer='Adam',loss='mean_squared_error')
model.fit(x_train, y_train,batch_size=128, epochs=100,validation_split=0.1)

# Get coefficients (weights) and corresponding feature

#df.drop('Thermal sensation',axis=1,inplace=True)
from sklearn.metrics import mean_squared_error

y_pred=model.predict(x_test)
loss=mean_squared_error(y_pred,y_test)
print('test loss:',loss)

from sklearn.model_selection import KFold
import numpy as np
from sklearn.metrics import mean_squared_error

kf = KFold(n_splits=5, shuffle=True, random_state=42)
mae_scores = []

for train_index, test_index in kf.split(X):
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]

    # Fit the model
    model.fit(X_train, y_train)

    # Make predictions on the test set
    y_pred = model.predict(X_test)

    # Compute Mean Squared Error (MSE)
    mae = mean_squared_error(y_test, y_pred)
    mae_scores.append(mae)

# Display MSE scores for each fold
for i, mse in enumerate(mae_scores):
    print(f'Fold {i+1} MSE: {mae}')

# Average MSE across all folds
average_mae = np.mean(mae_scores)
print(f'Average MAE: {average_mae}')

#!pip install scikit-optimize

import pickle

# Save the model
with open('model1.pkl', 'wb') as model_file:
    pickle.dump(model, model_file)

with open('pruned.pkl', 'rb') as model_file:
    loaded_model = pickle.load(model_file)

df.drop('Thermal sensation',inplace=True,axis=1)

input_data=df.sample(100)

input_data

# Warm-up runs
for _ in range(5):
    pruned_tree.predict(input_data) #warm up the pretrained model

import time

# Measure inference time
num_runs = 200# Number of inference runs to measure
inference_times = []

for _ in range(num_runs):
    start_time = time.time()
    pruned_tree.predict(input_data)
    end_time = time.time()
    inference_time = end_time - start_time
    inference_times.append(inference_time)

# Calculate statistics
average_inference_time = sum(inference_times) / num_runs
min_inference_time = min(inference_times)
max_inference_time = max(inference_times)

print(f"Average Inference Time: {average_inference_time} seconds")
print(f"Min Inference Time: {min_inference_time} seconds")
print(f"Max Inference Time: {max_inference_time} seconds")

import psutil

# CPU Usage
cpu_percent = psutil.cpu_percent(interval=1)

# Memory Usage
memory_info = psutil.virtual_memory()
total_memory_mb = memory_info.total / (1024 ** 2)
available_memory_mb = memory_info.available / (1024 ** 2)
used_memory_mb = memory_info.used / (1024 ** 2)

# Disk Usage (for the root directory '/')
disk_info = psutil.disk_usage('/')
total_disk_space_mb = disk_info.total / (1024 ** 2)
used_disk_space_mb = disk_info.used / (1024 ** 2)
free_disk_space_mb = disk_info.free / (1024 ** 2)

# Network Usage (bytes sent and received)
network_info = psutil.net_io_counters()
bytes_sent_mb = network_info.bytes_sent / (1024 ** 2)
bytes_received_mb = network_info.bytes_recv / (1024 ** 2)

# Print the results
print(f"CPU Usage: {cpu_percent}%")
print(f"Total Memory: {total_memory_mb:.2f} MB")
print(f"Available Memory: {available_memory_mb:.2f} MB")
print(f"Used Memory: {used_memory_mb:.2f} MB")
print(f"Total Disk Space: {total_disk_space_mb:.2f} MB")
print(f"Used Disk Space: {used_disk_space_mb:.2f} MB")
print(f"Free Disk Space: {free_disk_space_mb:.2f} MB")
print(f"Bytes Sent: {bytes_sent_mb:.2f} MB")
print(f"Bytes Received: {bytes_received_mb:.2f} MB")

import psutil

# Get the initial memory usage
#initial_memory = psutil.virtual_memory().used / (1024 ** 2)  # in MB

# Code for running your model
# ...

# Get the memory usage after running your model
final_memory = psutil.virtual_memory().used / (1024 ** 2)  # in MB

# Calculate the memory usage increase during model execution
memory_usage_increase = final_memory - initial_memory

print(f"Initial Memory Usage: {initial_memory:.2f} MB")
print(f"Final Memory Usage: {final_memory:.2f} MB")
print(f"Memory Usage Increase: {memory_usage_increase:.2f} MB")

'''
CPU Usage: 18.1%
Total Memory: 12982.62 MB
Available Memory: 11190.64 MB
Used Memory: 1502.75 MB
Total Disk Space: 110300.25 MB
Used Disk Space: 26914.14 MB
Free Disk Space: 83370.11 MB
Bytes Sent: 33.91 MB
Bytes Received: 65.58 MB
'''

